Notes:
    Data 
        Some market capitalizations are rounded to billion. 
        Market Capitalization is for non-affiliates based on closing price on last business day of second quarter 

        Construct isomorphism: 10ks <-> integer id <-> embeddings
        Construct training and development data in the form .conll file with structure
        integerid marketcap
        integerid marketcap
        integerid marketcap

        Args specifies path to training, development data & type of cell to be used 

        Market Caps are nominal - need to adjust for inflation
        Some market caps are non-USD denominated
        Extensive data wrangling for cleaning 10k files. Anomalies for extraction of market cap values many. Time consuming infrastructure work. 


    Tensorflow
        apply dropout to between layers not horizontally between steps
        probably don't need n_features  - we only have one feature: the doc vector.
        Max length refers to the length of "sentence", in our case the length of the sequence of the doc vectors aka the number of 10ks availaible for a company. 
        ""the optimal size of the hidden layer is usually between the size of the input and size of the output layers"
        "the situations in which performance improves with a second (or third, etc.) hidden layer are very small. One hidden layer is sufficient for the large majority of problems."


        RNN on words in document. Should be enough for a sizeble network. 



we are kind of screwed 

turn into classification problem 

LSTM predicts 

average glove vectors

two layer relu feedforward (200, 200)

test on company haven't seen before in testing

might want to use glove word vectors 

    average them for a document


    

from Kaparthy:

Monitoring Validation Loss vs. Training Loss

If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:

If your training loss is much lower than validation loss then this means the network might be overfitting. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.
If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)
Approximate number of parameters

The two most important parameters that control the model are rnn_size and num_layers. I would advise that you always use num_layers of either 2/3. The rnn_size can be adjusted based on how much data you have. The two important quantities to keep track of here are:

The number of parameters in your model. This is printed when you start training.
The size of your dataset. 1MB file is approximately 1 million characters.
These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:

I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make rnn_size larger.
I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.
Best models strategy

The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.

It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.

By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.

